#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Incrementally fill gas fields for Uniswap v3 event rows.

This script mirrors `add_origin.py` but fetches:
  ‚Ä¢ gasUsed                (from transaction receipt)
  ‚Ä¢ gasPrice               (from transaction; may be None on EIP-1559 txs)
  ‚Ä¢ effectiveGasPrice      (from transaction receipt; actual paid price)

It:
  - Reads an existing CSV (or continues from a partially-filled OUTPUT_CSV)
  - Finds rows with missing gas fields (any of the three)
  - Resolves unique tx hashes in parallel with multi-endpoint failover
  - Periodically checkpoints progress + writes out partial CSV
  - Is safe to resume after interruption
  - **Checks free disk space (>=30%) every time it saves a checkpoint; if below, exits**

CLI
---
python add_gas_fields.py \
  --in  /path/to/input.csv \
  --out /path/to/output.csv \
  --checkpoint /path/to/checkpoint.json \
  --batch-size 200 --workers 10 --save-every 1000
"""

import os
import json
import time
import argparse
import shutil
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
from web3 import Web3
from eth_defi.provider.multi_provider import create_multi_provider_web3


# ---------------- Disk space guard ----------------
MIN_FREE_RATIO = 0.30  # 30%

def _check_disk_space_for_path(path: str, min_free_ratio: float = MIN_FREE_RATIO) -> None:
    """
    Check free space on the filesystem containing `path`.
    If free/total < min_free_ratio, terminate the script.
    """
    dir_path = os.path.abspath(os.path.dirname(path) or ".")
    try:
        usage = shutil.disk_usage(dir_path)
        free_ratio = usage.free / usage.total if usage.total else 1.0
    except Exception as e:
        # If we cannot determine disk usage, fail safe and abort.
        print(f"üõë Could not determine disk usage for '{dir_path}': {e}")
        raise SystemExit(3)

    if free_ratio < min_free_ratio:
        print(
            f"üõë Low disk space on '{dir_path}': {free_ratio*100:.1f}% free "
            f"(threshold {min_free_ratio*100:.0f}%). Terminating to protect data integrity."
        )
        raise SystemExit(3)


# ---------------- RPC configuration ----------------
def build_mainnet_config(urls, timeout=5.0) -> str:
    """Return a space-joined string of endpoints that respond on chain_id==1."""
    ok = []
    for u in urls:
        try:
            tmp = Web3(Web3.HTTPProvider(u, request_kwargs={"timeout": timeout}))
            if tmp.eth.chain_id == 1:
                ok.append(u)
        except Exception:
            continue
    if not ok:
        raise RuntimeError("No Ethereum mainnet endpoints are reachable.")
    return " ".join(ok)


DEFAULT_RPC_URLS = [
    "https://eth.llamarpc.com/sk_llama_252714c1e64c9873e3b21ff94d7f1a3f",
    "https://mainnet.infura.io/v3/5f38fb376e0548c8a828112252a6a588",
    "https://snowy-broken-spring.quiknode.pro/e1c35cbb709b1cb095d49e42dcd4d40e6cbbfd7a",
    "https://eth.rpc.grove.city/v1/887ffda2",
    "https://lb.nodies.app/v1/c6a2e72646e34fc78d95513a52c4aca6",
]
JSON_RPC_LINE = build_mainnet_config(DEFAULT_RPC_URLS)
w3 = create_multi_provider_web3(JSON_RPC_LINE, request_kwargs={"timeout": 30.0})
assert w3.eth.chain_id == 1, "Connected chain is not Ethereum mainnet"


# ---------------- Defaults (can be overridden via CLI) ----------------
DEFAULT_INPUT_CSV = "/home/daniele/repositories/ABM_Uni_v3/data/usdc_weth_05.csv"
DEFAULT_OUTPUT_CSV = "/home/daniele/repositories/ABM_Uni_v3/data/univ3_pool_events_with_gas.csv"
DEFAULT_CHECKPOINT = "/home/daniele/repositories/ABM_Uni_v3/data/gas_fetch_checkpoint.json"

DEFAULT_BATCH_SIZE = 450
DEFAULT_WORKERS = 12
DEFAULT_SAVE_EVERY = DEFAULT_BATCH_SIZE*5


# ---------------- In-memory caches ----------------
# tx hash -> {"gasUsed": int|None, "gasPrice": int|None, "effectiveGasPrice": int|None}
_tx_cache: Dict[str, Dict[str, Optional[int]]] = {}


# ---------------- Checkpointing ----------------
def load_checkpoint(path: str) -> Dict:
    if os.path.exists(path):
        with open(path, "r") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                data = {}
        return {
            "processed_tx_count": data.get("processed_tx_count", 0),
            "pending_tx_hashes": data.get("pending_tx_hashes", []),
            "tx_cache": data.get("tx_cache", {}),
        }
    return {"processed_tx_count": 0, "pending_tx_hashes": [], "tx_cache": {}}


def save_checkpoint(path: str, processed_tx_count: int, pending_tx_hashes: List[str]):
    # Disk guard: ensure enough free space on the checkpoint filesystem
    _check_disk_space_for_path(path)

    checkpoint = {
        "processed_tx_count": processed_tx_count,
        "pending_tx_hashes": pending_tx_hashes,
        "tx_cache": _tx_cache,
    }
    with open(path, "w") as f:
        json.dump(checkpoint, f)
    print(f"  üíæ Checkpoint saved ‚Äî processed {processed_tx_count} so far")


# ---------------- Utilities ----------------
def canonical_missing_str(series: pd.Series) -> pd.Series:
    """True where string-like field is missing/empty."""
    if series.dtype == object:
        s = series.astype(str).str.lower()
        return series.isna() | (series.astype(str).str.len() == 0) | s.isin(["none", "nan"])
    return series.isna()


def canonical_missing_num(series: pd.Series) -> pd.Series:
    """True where numeric field is missing/invalid (NaN or 0)."""
    if pd.api.types.is_numeric_dtype(series):
        return series.isna() | (series == 0)
    # Try coercion if object
    coerced = pd.to_numeric(series, errors="coerce")
    return coerced.isna() | (coerced == 0)


def ensure_gas_columns(df: pd.DataFrame) -> None:
    """Create gas columns if absent (as nullable Int64)."""
    for col in ["gasUsed", "gasPrice", "effectiveGasPrice"]:
        if col not in df.columns:
            df[col] = pd.Series([None] * len(df), dtype="Int64")


def determine_work(df: pd.DataFrame) -> List[str]:
    """List unique tx hashes that still need any gas field."""
    if "transactionHash" not in df.columns:
        raise ValueError("Input CSV must contain 'transactionHash' column")

    ensure_gas_columns(df)

    need = (
        canonical_missing_num(df["gasUsed"])
        | canonical_missing_num(df["gasPrice"])
        | canonical_missing_num(df["effectiveGasPrice"])
    ) & df["transactionHash"].notna()

    work = df.loc[need, "transactionHash"].astype(str).unique().tolist()
    return work


# ---------------- RPC fetchers ----------------
def fetch_tx_gas(tx_hash: str, retry_count: int = 3, backoff: float = 0.6) -> Dict[str, Optional[int]]:
    """Fetch gasUsed, gasPrice, effectiveGasPrice for a hash."""
    if tx_hash in _tx_cache:
        return _tx_cache[tx_hash]

    last_err = None
    for attempt in range(retry_count):
        try:
            tx = w3.eth.get_transaction(tx_hash)
            receipt = w3.eth.get_transaction_receipt(tx_hash)

            gas_used = int(receipt.get("gasUsed")) if receipt and receipt.get("gasUsed") is not None else None
            # Note: On EIP-1559 txs, tx.get('gasPrice') can be None. That's expected.
            gas_price = int(tx.get("gasPrice")) if tx and tx.get("gasPrice") is not None else None
            eff_gas_price = (
                int(receipt.get("effectiveGasPrice"))
                if receipt and receipt.get("effectiveGasPrice") is not None
                else None
            )

            result = {"gasUsed": gas_used, "gasPrice": gas_price, "effectiveGasPrice": eff_gas_price}
            _tx_cache[tx_hash] = result
            return result
        except Exception as e:  # noqa: BLE001
            last_err = e
            if attempt < retry_count - 1:
                time.sleep(backoff * (attempt + 1))
            else:
                print(f"    ‚ö†Ô∏è  Failed to fetch gas for {tx_hash}: {str(e)[:140]}")
                result = {"gasUsed": None, "gasPrice": None, "effectiveGasPrice": None}
                _tx_cache[tx_hash] = result
                return result
    # Should not reach here
    raise RuntimeError(f"Unexpected error: {last_err}")


def batch_fetch_gas(tx_hashes: List[str], workers: int) -> Dict[str, Dict[str, Optional[int]]]:
    """Parallel fetch with a small cache passthrough."""
    results: Dict[str, Dict[str, Optional[int]]] = {}
    to_fetch: List[str] = []

    for h in tx_hashes:
        if h in _tx_cache:
            results[h] = _tx_cache[h]
        else:
            to_fetch.append(h)

    if not to_fetch:
        return results

    with ThreadPoolExecutor(max_workers=workers) as executor:
        future_to_hash = {executor.submit(fetch_tx_gas, h): h for h in to_fetch}
        for fut in as_completed(future_to_hash):
            h = future_to_hash[fut]
            try:
                results[h] = fut.result()
            except Exception as e:  # noqa: BLE001
                print(f"    ‚ö†Ô∏è  Error in thread for {h}: {str(e)[:140]}")
                results[h] = {"gasUsed": None, "gasPrice": None, "effectiveGasPrice": None}

    return results


# ---------------- Orchestration ----------------
def add_gas_incremental(
    df: pd.DataFrame,
    work_tx_hashes: List[str],
    *,
    batch_size: int,
    workers: int,
    save_every: int,
    start_index: int,
    checkpoint_path: str,
    output_csv: str,
) -> Tuple[pd.DataFrame, int]:
    """Fill gas fields for queued hashes from start_index. Returns (df, processed_tx_total)."""
    total = len(work_tx_hashes)
    print(f"\nüì• Pending unique tx hashes to resolve: {total}")

    processed_tx = start_index
    for i in range(start_index, total, batch_size):
        batch = work_tx_hashes[i : i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (total + batch_size - 1) // batch_size
        print(f"  üîÑ Batch {batch_num}/{total_batches} ‚Äî {len(batch)} txs")

        # Fetch
        fetched = batch_fetch_gas(batch, workers=workers)

        # Update only rows that still need any gas field
        need_mask = (
            (canonical_missing_num(df["gasUsed"])
             | canonical_missing_num(df["gasPrice"])
             | canonical_missing_num(df["effectiveGasPrice"]))
            & df["transactionHash"].isin(batch)
        )

        if need_mask.any():
            # Map per-column to keep nullable integers intact
            df.loc[need_mask, "gasUsed"] = df.loc[need_mask, "transactionHash"].map(
                lambda h: fetched.get(h, {}).get("gasUsed")
            )
            df.loc[need_mask, "gasPrice"] = df.loc[need_mask, "transactionHash"].map(
                lambda h: fetched.get(h, {}).get("gasPrice")
            )
            df.loc[need_mask, "effectiveGasPrice"] = df.loc[need_mask, "transactionHash"].map(
                lambda h: fetched.get(h, {}).get("effectiveGasPrice")
            )

        processed_tx = i + len(batch)

        if processed_tx % save_every == 0:
            # This will abort if disk space < 30% BEFORE writing checkpoint/CSV
            save_checkpoint(checkpoint_path, processed_tx, work_tx_hashes[processed_tx:])
            # If we made it past the checkpoint, go ahead and write the partial CSV
            df.to_csv(output_csv, index=False)
            print(f"  üíΩ Partial save ‚Üí {output_csv}")

    return df, processed_tx


# ---------------- Main ----------------
def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Fill gasUsed / gasPrice / effectiveGasPrice via RPC")
    p.add_argument("--in", dest="input_csv", default=DEFAULT_INPUT_CSV, help="Input CSV path")
    p.add_argument("--out", dest="output_csv", default=DEFAULT_OUTPUT_CSV, help="Output CSV path")
    p.add_argument("--checkpoint", dest="checkpoint", default=DEFAULT_CHECKPOINT, help="Checkpoint JSON path")
    p.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE)
    p.add_argument("--workers", type=int, default=DEFAULT_WORKERS)
    p.add_argument("--save-every", type=int, default=DEFAULT_SAVE_EVERY)
    return p.parse_args()


def main():
    args = parse_args()

    input_csv = args.input_csv
    output_csv = args.output_csv
    checkpoint_path = args.checkpoint
    batch_size = args.batch_size
    workers = args.workers
    save_every = args.save_every

    print("üöÄ Starting gas fetch (incremental-safe) ...")
    print(f"üìÑ INPUT_CSV  = {input_csv}")
    print(f"üìÑ OUTPUT_CSV = {output_csv}")
    print(f"üß∑ CHECKPOINT = {checkpoint_path}")
    print(f"‚öôÔ∏è  batch={batch_size}  workers={workers}  save_every={save_every}")

    # Load checkpoint & cache
    checkpoint = load_checkpoint(checkpoint_path)
    global _tx_cache
    _tx_cache = checkpoint.get("tx_cache", {}) or {}
    if _tx_cache:
        print(f"‚ôªÔ∏è  Loaded cache with {len(_tx_cache)} tx entries")

    # Choose base dataframe
    if os.path.exists(output_csv):
        print("üìÇ Found existing OUTPUT_CSV. Continuing to fill missing gas fields ...")
        df = pd.read_csv(output_csv)
    else:
        print("üìÇ No OUTPUT_CSV found. Loading INPUT_CSV and starting from scratch ...")
        df = pd.read_csv(input_csv)

    ensure_gas_columns(df)

    # Worklist (checkpoint or fresh)
    if checkpoint.get("pending_tx_hashes"):
        work_tx_hashes = checkpoint["pending_tx_hashes"]
        start_index = checkpoint.get("processed_tx_count", 0)
        print(f"üîÅ Resuming: {start_index} processed / {len(work_tx_hashes)} pending")
    else:
        work_tx_hashes = determine_work(df)
        start_index = 0
        if not work_tx_hashes:
            print("‚úÖ Nothing to do ‚Äî all gas fields already present.")
            # Still write out (ensures unified schema)
            # Extra safety: ensure disk is okay before final write
            _check_disk_space_for_path(output_csv)
            df.to_csv(output_csv, index=False)
            return

    t0 = time.time()
    df, processed_tx = add_gas_incremental(
        df,
        work_tx_hashes,
        batch_size=batch_size,
        workers=workers,
        save_every=save_every,
        start_index=start_index,
        checkpoint_path=checkpoint_path,
        output_csv=output_csv,
    )

    # Final save (guarded)
    print(f"\nüíæ Saving CSV to {output_csv} ...")
    _check_disk_space_for_path(output_csv)
    df.to_csv(output_csv, index=False)

    # Stats
    elapsed = time.time() - t0
    total_rows = len(df)
    ok_gas_used = df["gasUsed"].notna().sum()
    ok_gas_price = df["gasPrice"].notna().sum()
    ok_eff = df["effectiveGasPrice"].notna().sum()

    print("\n" + "=" * 60)
    print("‚úÖ COMPLETED!")
    print(f"üìä Total rows: {total_rows}")
    print(f"‚úÖ gasUsed filled: {ok_gas_used}")
    print(f"‚úÖ gasPrice filled: {ok_gas_price}  (may be None on EIP-1559 txs)")
    print(f"‚úÖ effectiveGasPrice filled: {ok_eff}")
    print(f"‚è±Ô∏è  Time elapsed: {elapsed:.2f} s")
    print(f"üìÑ Output saved to: {output_csv}")
    print("=" * 60)

    # Cleanup checkpoint
    if os.path.exists(checkpoint_path):
        os.remove(checkpoint_path)
        print("üßπ Checkpoint file removed")


if __name__ == "__main__":
    main()
